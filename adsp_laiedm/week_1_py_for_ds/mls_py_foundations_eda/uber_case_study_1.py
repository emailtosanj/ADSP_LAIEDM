# -*- coding: utf-8 -*-
"""Uber_Case_Study-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F8XzPaguYjbO534xKS7JlDdyJcfwamHV

-------------------------------------
# **Uber Data Analysis**
-------------------------------------

--------------------
## **Context**
--------------------

Uber Technologies, Inc. is an American multinational transportation network company based in San Francisco and has operations in approximately 72 countries and 10,500 cities. In the fourth quarter of 2021, Uber had 118 million monthly active users worldwide and generated an average of 19 million trips per day.

Ridesharing is a very volatile market and demand fluctuates wildly with time, place, weather, local events, etc. The key to being successful in this business is to be able to detect patterns in these fluctuations and cater to the demand at any given time.

As a newly hired Data Scientist in Uber's New York Office, you have been given the task of extracting insights from data that will help the business better understand the demand profile and take appropriate actions to drive better outcomes for the business. Your goal is to identify good insights that are potentially actionable, i.e., the business can do something with it.

------------------
## **Objective**
------------------

To extract actionable insights around demand patterns across various factors.

-----------------------------
## **Key Questions**
-----------------------------

1. What are the different variables that influence pickups?
2. Which factor affects the pickups the most? What could be plausible reasons for that?
3. What are your recommendations to Uber management to capitalize on fluctuating demand?

------------------------------------
## **Dataset Description**
------------------------------------

The data contains information about the weather, location, and pickups.

* pickup_dt: Date and time of the pick-up
* borough: NYC's borough
* pickups: Number of pickups for the period (1 hour)
* spd: Wind speed in miles/hour
* vsb: Visibility in miles to the nearest tenth
* temp: Temperature in Fahrenheit
* dewp: Dew point in Fahrenheit
* slp: Sea level pressure
* pcp01: 1-hour liquid precipitation
* pcp06: 6-hour liquid precipitation
* pcp24: 24-hour liquid precipitation
* sd: Snow depth in inches
* hday: Being a holiday (Y) or not (N)

##  **Importing the necessary libraries and overview of the dataset**
"""

# Library to suppress warnings
import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
# Libraries to help with reading and manipulating data
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Libraries to help with data visualization
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

# Library to extract datetime features
import datetime as dt

from google.colab import drive
drive.mount('/content/drive')

"""### **Loading the dataset**"""

from posix import listdir
import os
file_dir = os.getcwd()+'/drive/MyDrive/adsp_laiedm/week_1_py_for_ds/mls_py_foundations_eda'
file_lst_dir = os.listdir(os.getcwd()+'/drive/MyDrive/adsp_laiedm/week_1_py_for_ds/mls_py_foundations_eda')
print(file_dir)

data = pd.read_csv('/content/drive/MyDrive/adsp_laiedm/week_1_py_for_ds/mls_py_foundations_eda/Uber.csv')

# Copying data to another variable to avoid any changes to the original data
df = data.copy()

data.head()

"""### **View the first 5 rows of the dataset**"""

# Looking at head (the first 5 observations)
df.head()

"""**Observations:**

* The column pickup_dt includes the pickup date and time. The date shows that the data starts from 01-Jan-2015.
* The column borough contains the name of the New York borough in which the pickup was made.
* The column pickups contain the number of pickups in the borough at the given time.
* All of the weather variables are numerical.
* The variable holiday is a categorical variable.

### **View the last 5 rows of the dataset**
"""

# Looking at tail (the last 5 observations)
df.tail()

"""**Observations:**

* The head indicated that the data began on January 1, 2015, whereas the tail indicatesÂ that it continued until June 30, 2015. This means we have **six months' worth of data to analyze**.

### **Checking the shape of the dataset**
"""

df.shape

"""* The dataset has **29,101 rows and 13 columns**.

### **Checking the info()**
"""

df.info()

"""**Observations:**

* All columns have 29,101 observations except borough, which has 26,058 observations indicating that there are null values in it.
* pickup_dt is read as an 'object' data type, but it should have the data type as DateTime.
* borough and hday (holiday) should be categorical variables.

### **Summary of the data**
"""

df.describe().T

"""* There is a significant discrepancy between the third quartile and the highest value for the number of pickups (pickups) and the snow depth (sd), indicating that these variables may have outliers to the right.
* The temperature has a broad range, showing that the data includes records from the winter as well as summer seasons.

**By default, the describe() function shows the summary of numeric variables only. Let's check the summary of non-numeric variables.**
"""

df.describe(exclude = 'number').T

"""**Observations:**

* The variable 'borough' has six unique categories. The category Bronx has occurred 4,343 times in the data.
* The variable 'hday' has 2 unique categories. The category N, i.e., not a holiday as occurred more often, which makes sense.

**Let's check the count of each unique category in each of the categorical variables.**
"""

# Making a list of all categorical variables
cat_col = ['borough', 'hday']

# Printing number of count of each unique value in each column
for column in cat_col:
    print(df[column].value_counts())

    print('-' * 50)

"""* The above output shows that the borough variable has an equal count for each category.

### **Extracting date parts from pickup date**
"""

# Converting pickup_dt datatype to datetime
df.pickup_dt = pd.to_datetime(df.pickup_dt)

# Extracting date parts from pickup_dt
df['start_year'] = df.pickup_dt.dt.year

df['start_month'] = df.pickup_dt.dt.month_name()

df['start_hour'] = df.pickup_dt.dt.hour

df['start_day'] = df.pickup_dt.dt.day

df['week_day'] = df.pickup_dt.dt.day_name()

# Removing pickup_dt column as it will not be required for further analysis
df.drop('pickup_dt', axis = 1, inplace = True)

df.info()

"""### **Missing value treatment**"""

# Checking missing values
df.isna().sum()

"""* There are 3043 missing values for the variable borough.
* Other variables have no missing values.
"""

df.head()

# Checking the missing values further
df.borough.value_counts(normalize = False, dropna = False)

"""* All the six categories have the same percentage, i.e., ~15%. There is no mode (or multiple modes) for this variable.
* The percentage of missing values is close to the percentage of observations from other boroughs.
* We can treat the missing values as a separate category for this variable.
"""

# Replacing NaN with Unknown
df['borough'].fillna('Unknown', inplace = True)

df.borough.value_counts()

df.isnull().sum()

"""* Now, there are no missing values in the data.

## **Exploratory Data Analysis: Univariate**

**Let us explore the numerical variables first.**
"""

import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

def histogram_boxplot(feature, figsize=(15, 10), bins="auto"):
    """ Boxplot and histogram combined
    feature: 1-d feature array
    figsize: size of fig (default (15, 10))
    bins: number of bins (default "auto")
    """
    f, (ax_box, ax_hist) = plt.subplots(
        nrows=2,  # Number of rows of the subplot grid
        sharex=True,  # The X-axis will be shared among all the subplots
        gridspec_kw={"height_ratios": (.25, .75)},
        figsize=figsize
    )

    # Creating the subplots
    # Boxplot will be created and the mean value of the column will be indicated using some symbol
    sns.boxplot(x=feature, ax=ax_box, showmeans=True, color='red')

    # For histogram
    sns.histplot(x=feature, kde=False, ax=ax_hist, bins=bins)
    ax_hist.axvline(np.mean(feature), color='g', linestyle='--')      # Add mean to the histogram
    ax_hist.axvline(np.median(feature), color='black', linestyle='-') # Add median to the histogram

    plt.show()

"""### **Observations on Pickups**"""

histogram_boxplot(df.pickups)

"""**Observations:**
* The distribution of hourly pickups is highly right-skewed.
* The majority of the hourly pickups are close to 0.
* Median pickups are equal to 0, but the mean is ~500.
* There are a lot of outliers in this variable.
* While most hourly pickups are at the lower end, we have observations where hourly pickups went as high as 8000.

###  **Observations on Visibility**
"""

histogram_boxplot(df.vsb)

"""**Observations:**
* The distribution of 'visibility' is left-skewed.
* Both the mean and the median are high, indicating that the visibility is good on most days.
* There are, however, outliers towards the left, indicating that visibility is extremely low on some days.
* It will be interesting to see how visibility affects the Uber pickup frequency.

### **Observations on Snow Depth**
"""

histogram_boxplot(df.sd)

"""**Observations:**
* We observe that there is a snowfall in the period that we are analyzing.
* There are outliers in this variable.
* We will have to see how snowfall affects pickups. We know that very few people are likely to get out if it is snowing heavily, so our pickups would likely decrease when it snows.

**Now, let's explore the categorical variables.**
"""

def bar_perc(data, z):
    total = len(data[z]) # Length of the column
    plt.figure(figsize = (15, 5))

    # Convert the column to a categorical data type
    data[z] = data[z].astype('category')

    ax = sns.countplot(x=z, data=data, palette='Paired', order=data[z].value_counts().index)

    for p in ax.patches:
        percentage = '{:.1f}%'.format(100 * p.get_height() / total) # Percentage of each class
        x = p.get_x() + p.get_width() / 2 - 0.05                    # Width of the plot
        y = p.get_y() + p.get_height()                              # Height of the plot
        ax.annotate(percentage, (x, y), size = 12)                  # Annotate the percentage

    plt.show()                                                      # Display the plot

"""### **Observations on holiday**"""

bar_perc(df, 'hday')

"""**Observation:**

* Only 3.9% of days were holidays in the period that we are analyzing.

### **Observations on borough**
"""

bar_perc(df, 'borough')

"""**Observation:**

* The observations are uniformly distributed across the boroughs except for the observations that had NaN values and were attributed to the Unknown borough.

## **Exploratory Data Analysis: Multivariate**

**Let's plot multivariate charts between variables to understand their interaction with each other.**

### Correlation
"""

# Check for correlation among numerical variables
num_var = ['pickups', 'spd', 'vsb', 'temp', 'dewp', 'slp', 'pcp01', 'pcp06', 'pcp24', 'sd']
corr = df[num_var].corr()

# Plot the heatmap
plt.figure(figsize = (14, 10))
sns.heatmap(corr, annot = True, cmap = 'coolwarm',
        fmt = ".1f",
        xticklabels = corr.columns,
        yticklabels = corr.columns)

"""**Observations:**

* Dew point is an indication of humidity, which is correlated with temperature and the same thing can be observed in the heatmap as well. Temperature shows a high correlation with dew point.
* Visibility is negatively correlated with precipitation. If the rains are high during the hour, visibility is low. This is aligned with our intuitive understanding.
* Snow depth, of course, would be negatively correlated with the temperature.
* The wind speed and the sea level pressure are negatively correlated with the temperature.
* It is important to note that correlation does not imply causation.
* There does not seem to be a strong relationship between the number of pickups and weather stats.

## **Relationship between pickups and time based variables**

### **Pickups across Months**
"""

cats = df.start_month.unique().tolist()
df.start_month = pd.Categorical(df.start_month, ordered = True, categories = cats)
plt.figure(figsize = (20, 7))
sns.lineplot(x = "start_month", y = "pickups", data = df, ci = 0, color = "RED", estimator = 'sum')
plt.ylabel('Total pickups')
plt.xlabel('Month')
plt.show()

"""**Observations:**
* There is a clear increasing trend in monthly bookings.
* Bookings in June are almost 1.5 times that of Jan.

### **Pickups vs Days of the Month**
"""

plt.figure(figsize = (20, 7))
sns.lineplot(x = "start_day", y = "pickups", estimator = 'sum', ci = 0, data = df, color = "RED")
plt.ylabel('Total pickups')
plt.xlabel('Day of Month')
plt.show()

"""**Observations:**
* Number of pickups are low towards the end of the month (29th - 31st).
* Number of pickups for 31 might be low because not all months have the 31st day.
* There is a peak in the bookings around the 20th day of the month.

### **Pickups across Weekdays**
"""

cats = ['Monday', 'Tuesday', 'Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']
df.week_day = pd.Categorical(df.week_day, ordered = True, categories = cats)
plt.figure(figsize = (20, 7))
sns.lineplot(x = "week_day", y = "pickups", ci = 0, data = df, color = "RED")
plt.ylabel('Mean pickups')
plt.xlabel('Weeks')
plt.show()

"""**Observations:**
* Pickups gradually increase as the week progresses and starts dropping after Saturday.
* We need to do more investigation to understand why the demand for Uber is low at the beginning of the week.

### **Pickups across Boroughs**
"""

plt.figure(figsize = (20, 10))
sns.boxplot(x='borough', y='pickups', data=df)
plt.ylabel('pickups')
plt.xlabel('Borough')
plt.show()

"""**Observations:**
* There is a clear difference in ridership across the different boroughs.
* Manhattan has the highest number of bookings.
* Brooklyn and Queens are distant followers.
* EWR, Unknown, and Staten Island have a very low number of bookings. The demand is so small that probably it can be covered by the drop-offs of the inbound trips from other areas.

### **Relationship between Pickups and Holidays**
"""

df.groupby('hday')['pickups'].mean()

# Check if the trend is similar across boroughs
df.groupby(by = ['borough','hday'])['pickups'].mean()

"""**Observations:**
1. The mean pickups on holidays are lesser than a non-holiday.
2. Except for Manhattan, mean pickups on holidays are pretty similar to non-holiday pickups.
3. In Queens, mean pickups on holidays are higher.
4. There are hardly any pickups in EWR.

### **Relationship between Pickups and Hour of the day across Boroughs**
"""

plt.figure(figsize = (20, 7))
sns.lineplot(x = "start_hour", y = "pickups", ci = 0, data = df, hue = 'borough')
plt.ylabel('Pickups')
plt.xlabel('Hour of the day')
plt.show()

"""**Observations:**

* Bookings peak around the 19th and 20th hour of the day and decreases till 5 AM.
* The peak can be attributed to the time people leave their workplaces.
* From 5 AM onwards, we can see an increasing trend till 10, possibly the office rush.
* Pickups go down from 10 AM to 12 PM post that they start increasing.
* The number of pickups in Manhattan is very high and dominant when we see the spread across boroughs.
* We cannot observe the distribution for EWR and Staten Island boroughs in this plot due to the very low count in these boroughs. Let's try converting the pickups on a logarithmic scale to visualize all the boroughs.
"""

plt.figure(figsize = (20, 7))
sns.lineplot(x = df.start_hour, y = np.log1p(df.pickups), estimator ='sum', ci = 0, hue = df.borough)
plt.ylabel('Total pickups')
plt.xlabel('Hour of the day')
plt.legend(bbox_to_anchor = (1, 1))
plt.show()

"""**Observations:**
* Hourly pattern can be seen in almost all the boroughs.
* After applying the logarithmic scale, it is obvious that the four major boroughs follow the same pattern.
* EWR seems to have a random demand with a majority of the values being zero.
* Manhattan sees the most Uber pickups. Let us explore this borough in more detail.

### Manhattan Pickups Heatmap - Weekday vs Hour
"""

df_man = df[df.borough == 'Manhattan']
df_hm = df_man.pivot_table(index = 'start_hour', columns = 'week_day', values = 'pickups')

# Draw a heatmap
plt.figure(figsize = (20, 10)) # To resize the plot
sns.heatmap(df_hm,  fmt = "d", cmap = 'coolwarm', linewidths = .5, vmin = 0)
plt.show()

"""**Observations:**

* The demand for Uber peaks during the late hours of the day when people are returning home from the office.
* Demand continues to be high during the late hours of the day (midnight) on Fridays and Saturdays.  
* It is odd that the demand for Uber is not as high on Monday evenings in comparison to other working days.

## **Conclusion and Recommendations**

-----------------------------------------------------------------
### **Conclusion**
-----------------------------------------------------------------

We analyzed a dataset of nearly 30K hourly Uber pickup information, from New York boroughs.
The data spanned every day of the first six months of the year 2015.
The main feature of interest here is the number of pickups.
From an environmental and business perspective, having cars roaming in an area while the demand is in another or filling the streets with cars during a low demand period while lacking during peak hours is inefficient. Thus, we determined the factors that affect pickup and the nature of their effect.

We have been able to conclude that:

1. Uber cabs are most popular in the Manhattan area of New York.
2. Contrary to intuition, weather conditions do not have much impact on the number of Uber pickups.
3. The demand for Uber has been increasing steadily over the months (Jan to June).
4. The rate of pickups is higher on the weekends in comparison to weekdays.
5. It is encouraging to see that New Yorkers trust Uber taxi services when they step out to enjoy their evenings.
6. We can also conclude that people use Uber for regular office commutes. The demand steadily increases from 6 AM to 10 AM, then declines a little and starts picking up till midnight. The demand peaks at 7-8 PM.
7. We need to further investigate the low demand for Uber on Mondays.

--------------------------------------------------
### **Recommendation to business**
--------------------------------------------------

1. Manhattan is the most mature market for Uber. Brooklyn, Queens, and Bronx show potential.
2. There has been a gradual increase in Uber rides over the last few months, and we need to keep up the momentum.
3. Riderships are high at peak office commute hours on weekdays and during late evenings on Saturdays. Cab availability must be ensured during these times.
4. The demand for cabs is the highest on Saturday nights. Cab availability must be ensured during this time of the week.
5. Procure data for fleet size availability to get a better understanding of the demand-supply status and build a machine learning model to accurately predict pickups per hour, to optimize the cab fleet in respective areas.
6. Procure more data on price and build a model that can predict optimal pricing.

---------------------------------
###  **Further Analysis**
---------------------------------
1. Dig deeper to explore the variation of cab demand, during working days and non-working days. You can combine Weekends+Holidays to be non-working days and weekdays to be the working days.
2. Drop the boroughs that have negligible pickups and then analyze the data to uncover more insights.
"""